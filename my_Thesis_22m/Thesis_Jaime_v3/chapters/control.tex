\chapter{Minimization on Torque Level for an 8-DOF Robot}
\label{ch:control}





\section{Projected Gradient Minimization}
\label{sec:Gradientbasedminimization}

As it was introduced in Section \ref{subsec:nsprojection}, the null space projector $\mathrm{\mathbf{N_{\tau}}} = \mathrm{(\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T})}$ may be used to project torques into the null space of the main task. 
 
 
 
 
 
 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
 
\textbf{ THE FOLLOWING APPLIES SO FAR ONLY TO VELOCITIES:} \\
\TODO{USING NEMEC-MINE version...If the scaling gain is high it  oscillates around a point that is not a minimum. But with small gains it optimizes fine though...DOES THE CONDITION OF SPDness DO NOT APPLY TO THE PROJECTOR IN THE TORQUES??? In the papers I found it always refers to joint level :/ } 
\correction{in matlab postmultiplying by $M$ or premultiplying by $M^{-1}$ return P.D. matrices numerically tested} \\
\correction{if I cannot prove just say that the prerotation always applies for the velocity. All the papers where they are used (only in velocity level) refer to the Foundations of robotics of Yoshikawa to which I do not have acces.}
%
\textcolor{magenta}{ This null space may be used to optimize some specific function $\mathrm{{p}={p}(\mathbf{q})}$. Now let us define its gradient $ \mathrm{\mathbf{\phi} =  \nabla {p}(\mathbf{q})}$ and a scaling factor  $k$. According to \cite{yoshikawa}, $\mathrm{p}$ is minimized for any scalar $k < 0$, and maximized for any scalar $k > 0$. This holds	as long as $\mathrm{\phi^{T} \mathbf{N} \phi}$ is semidefinite positive, being $\mathrm{\mathbf{N}}$ the null space projector, either in torque or velocity level. For the Moor-Penrose pseudoinverse this is true since the the null space projector is positive definite. For the mass weighted pseudoinverse the projector is non-definite since it is not symmetrical. Using the mass weighted pseudoinverse, the null space projector in for the joint velocities ($\mathrm{\mathbf{N_{\dot{q}}}}$) may be forced to be positive definite, by postmuliplying it by $\mathrm{\mathbf{M^{-1}}}$ as proposed in \cite{Nemec}. This ensures the best optimization step for the mass weighted pseudoinverse.} \\
 
 



In torque level a different matrix must be taken to force positive definiteness. Let us take the mass matrix  $\mathrm{\mathbf{M}}$ instead.




\begin{lemma}
	Given the null space projector   $\mathrm{\mathbf{N_{\tau}}} = \mathrm{(\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T})}$, where $\mathrm{\mathbf{J}^{\#}}$ is the mass weighted pseudoinverse \cite{khatib1995}. Then the form
	\begin{equation}
	\phi^{T} \mathbf{N_{\tau}} \mathbf{M} \phi
	\label{eq:nemec_torque}
	\end{equation}  is positive semi-definite.
\end{lemma}

\begin{proof}
	First the symmetry is to be proven.
	\begin{equation}
	\begin{aligned}
	\phi^{T} (\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T}) \mathbf{M} \phi = \\
	\phi^{T} (\mathbf{M} - \mathbf{J}^T 
	\mathbf{ (J M^{-1} J^{T})^{-1}	J  M^{-1}	}\mathbf{M} ) \phi .
	\end{aligned}	
	\end{equation}
	
	The term $\mathrm{(\mathbf{M} - \mathbf{J}^T \mathbf{ (J M^{-1} J^{T})^{-1}	J })}$ is always symmetrical since $\mathrm{\mathbf{M}}$ is symmetrical.
\end{proof}

\begin{proof}
	Now we have to prove that (\ref{eq:nemec_torque}) is positive semi-definite. For that the symmetric mass matrix is decomposed $\mathrm{\mathbf{M = U^{T} U}}$.
	%
	\begin{equation}
	\begin{aligned}
	\phi^{T} (\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T}) \mathbf{U^{T} U} \phi = \\
	\phi^{T} (\mathbf{U^{T} U} - \mathbf{J}^T 
	\mathbf{ (J (U^{T} U)^{-1} J^{T})^{-1}	J } )\phi = \\
	\phi^{T} (\mathbf{U^{T} U} - \mathbf{J}^T 
	\mathbf{ (J (U^{T} U)^{-1} J^{T})^{-1}	J } \mathbf{(U^{T} U)}^{-1}	\mathbf{(U^{T} U)}) \phi 
	\end{aligned}
	\end{equation}
	
	Substituting $z=J U^{T}$ and $z=J U^{T}$
	
	
\end{proof}
	


XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\


In the equation (\ref{eq:gbm_nico}) the  projector will minimize the reflected mass $\mathrm{m_u}$ inside the null space of the first task, as long as $\mathrm{k}$ is a positive scalar. 

In (\ref{eq:gbm_nico}) the gradient of the reflected mass was projected into the null space using the well known projector from \cite{khatib1995}. The same projector is used here in order to ensure static and dynamic consistency.  



%
\textbf{WITH THE NORMAL PROJECTOR:}\\
 As with the 7-DOF LWR, the 8-DOF robot shows bad performance close to local maxima. This is an expected result because the mass matrix, and the Jacobian, do not depend on the linear axis position (see (\ref{eq:j_m_no__q1})). The rotational joints are not able to overcome the friction even for high values of $k$. This problem is not present in the linear axis which has a position controller. The linear axis is included into the whole-body impedance control framework using an admittance interface \cite{whole_body_imp}. Therefore, even small values for the torque cause a displacement of this joint.
  However, this movement is oscillatory, specially for high gains. These oscillations appear around the starting position of the linear axis. Therefore, this joint is not used for the minimization, because the projection is done onto a self-motion manifold slice for a constant position of the linear axis.
%% 
% 

\textcolor{blue}{ SINCE THE COMPUTATION OF THE PSEUDOINVERSE IS EXPENSIVE MAYBE IT COULD BE DONE THE IMPOSITION OF THE CONSTRAINT FOR THE ACCELERATIONS. AS IT IS DONE LATER TO OBTAIN A NULL SPACE VELOCITY, BUT HERE USED TO OBTAIN A NULL SPACE TORQUE} \textcolor{red}{FIND OUT IF THIS IS POSSIBLE
}
%% HERE THE SADDLE POINTS DO NOT MAKE SENSE ANYMORE. And not only close to maxima, but close to saddle point, which makes sense as the null  problem is characteristic of two dimensional data. 



%One could think of obtaining new equations of motion for the robot, where the constraint of null space movement would be satisfied. Which using the Lagragian approach would be getting from 
%
%\begin{equation}
%M(\mathbf{q})\ddot{\mathbf{q}}+C(\mathbf{q},\dot{\mathbf{q}})+G(\mathbf{q})=\mathbf{\tau} +J_c*\mathbf{\Lambda},
%\label{joint_space_dynamics}
%\end{equation}	
%to	
%
%\begin{equation}
%\bar{M}(\mathbf{q})\ddot{\mathbf{q}}+\bar{C}(\mathbf{q},\dot{\mathbf{q}})+\bar{G}(\mathbf{q})=\bar{\mathbf{\tau}},
%\label{new_joint_space_dynamics}
%\end{equation}	
%
%where $J_c$ is the [6x8] constrained Jacobian that express the null space movement and $\Lambda$ are the lagrangian multipliers. But one can see that none of the matrices in the new equation of motion would depend on $q_1$ as none of the originals depends on $q_1$, so this would actually solve nothing.














\section{Minimization Based on Attractive Potential}
\label{sec:Minimattractivepotential}




For the potential field  defined in (\ref{eq:potential_intro}) the aim is to find a suitable goal position, where the reflected mass on the tip of the robot in certain direction is minimal. Two possible goal positions are: Global minimum, determining all possible null space positions and then finding the local minima and global minimum. This is treated in  Section \ref{sec:globaloptimization}. And local minimum: an iterative solution is discussed in  Section \ref{sec:localoptimization}.
Like it occurred with the 7-DOF LWR, since the goal position is chosen to be far from local maxima the problem from the previous approach will not appear here.

%\begin{equation}
%U(\mathbf{q}) = - \frac{1}{2} Kp (\mathbf{q}_{m_u}^\ast - \mathbf{q})^T (\mathbf{q}_{m_u}^\ast - \mathbf{q}). \label{eq:potential_extension8}
%\end{equation}

%Differentiating we obtain a torque to project into the null space
%
%\begin{equation}
%\mathbf{\tau}_{m_u}^\ast = \frac{\partial {U(\mathbf{q})}}{\partial {\mathbf{q}}} = - k (\mathbf{q}_{m_u}^\ast - \mathbf{q}).
%\end{equation}
%
%
%\begin{equation}
%\mathbf{\tau}_d = \mathbf{\tau}_\mathrm{imp} + (I - J^T J^{\#T}) \mathbf{\tau}_{m_u}^\ast, \label{eq:potential_controller_extension8}
%\end{equation}





%\section{Conclusion}
%\label{sec:conclusion_ext8}



\subsection{Global Optimization}
\label{ch:globaloptimization}
\label{sec:globaloptimization}


From \cite{fabianthesis} we already have the null space positions as a point cloud. This numerically obtained dataset is used as input in Section \ref{sec:global_numerical} for an algorithm that returns the local and global extrema of the grid. However, an analytical form of the grid is of interest. Therefore the Section \ref{sec:global_analytical} is dedicated to obtain an analytical form  of this null space grid.




\subsubsection{Analytically}
\label{sec:global_analytical}



%\subsection{Imposing the null space constraint in the reflected mass}
\label{subsec:imposing_ns_constraint}


The goal is to reformulate $m_u$, so it does include the null space constraint. 
For sake of simplicity the problem is explained for the 7-DOF LWR.  We aim to get $m_u(\mathbf{q}_{ns})$ where $\mathbf{q}_{ns}$ is


\begin{equation}
\mathbf{q}_{ns}(t) = \int_{t_0}^t \! \ker(\mathbf{J}(\mathbf{q}(\tau))) \, \mathrm{d}\tau. 
\label{eq:integral_kernel}
\end{equation}

For the 7-DOF LWR the analytical formula of the kernel of the Jacobian is:

\begin{equation}
\ker(\mathbf{J}(\mathbf{q}))  =\begin{pmatrix} -0.06084s_6 c_3 s_4^2\\
0. 6084s_6 s_4^2 s_3 s_2\\
0.00156s_4 s_6 (-40s_2)...\\
...-39s_2 c_4 +39c_2 c_3 s_4\\
0\\
-0.00156s_4 s_2 (-40c_4 s_6)+...\\
...40s_4 c_5 c_6 -39s_6\\
-0.06241s_6 s_4^2 s_2 s_5\\
0.0624s_4)^2 s_2 c_5	
\end{pmatrix}	,
\label{eq:dq_ns}
\end{equation}


where $s_i$ and $c_i$ are respectively the sinus and cosine of the $q_i$ joint. For this kernel of the Jacobian, the integral of the equation (\ref{eq:integral_kernel}) is analytically not solvable with software like Maple or Mathematica. So we cannot obtain a closed-form of the reflected mass, such that the null space constraint is imposed.\\

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\

Another way may be obtaining the anti-derivatives of the entries of (\ref{eq:dq_ns}). Let us take the last term of the fifth entry, $s_6$. From the numerical integration done in  \cite{fabianthesis} it is known that the sixth rotational joint takes the form of a sinus. Therefore this term can be rewriten as $\mathrm{sin(sin(x))}$.  
Mathematica is not able to return this integral as a composition of elementary functions because they are not built into the Wolfram Language \cite{wolfram}. This function may however be represented as a suitably generalized Kampé de Fériet hypergeometric function \cite{kampe} of two variables.
 
\textcolor{red}{ARE THE OTHER FUNCTIONS ALSO INTEGRABLE??}
%

\textbf{THE REST I GUESS IS WORTHLESS BECAUSE THE ANTIDERIVATIVE IT ACTUALLY EXISTS:}\\
This function is Riemann integrable, but this does not mean that a closed-form of the integral exists. \\
Unfortunately $\mathrm{sin(sin(x))}$ has most likely no antiderivative. It can only be evaluated in specific intervals, for which tables, like the ones in \cite{hanbook_math}, provide a result depending on Struve and Bessel functions \cite{struve}. In the intervals where the integral cannot be evaluated, it may be representable as "incomplete" Struve and Bessel functions, which nowadays may only be approximately computed using special algorithms \cite{incomplete_bessel}. One may argue if these solutions for the definite integral are closed-form or not. In general a solution is said to be "closed-form" if it can be expressed analytically in terms of some "well-known" functions \cite{def_closed_form},  and nowadays not all the authors accept "incomplete" Struve functions as closed-form.  \\
Software like Mathematica compute elementary antiderivatives using approaches like the Risch algorithm \cite{risch}, which attempts to prove Liouville's theorem \cite{lioville} for the existence of antiderivatives as rational functions.  An explicit proof of the non-existence of this antiderivative  by hand is left to future work.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\





For our case with the 8-DOF robot, an attempt is done using curve fitting. If successful, this would generate a function, from this point cloud, that could be used as an input to the solvers provided by MATLAB Optimization Toolbox. But the attempts performed result always in a function that leads to a very different surface from the original  set of possible null space positions. The difficulty increases when the robot is following a trajectory because the surface changes with the direction of the end-effector. Being this approach discarded, one needs to work with numerical values. 





\subsubsection{Numerically}
\label{sec:global_numerical}


In general the minimization must be performed while the robot is following a trajectory. Knowing the trajectory a priori may allow the computation of the null space grids for the different configurations, and directions of the end-effector, during the motion. This implies that the computation of these grids and the search of the global minima must be offline. For a general trajectory, the time taken to compute the grids, precludes from online capabilities. To compute a rough grid with a large step size requires at least 0.1 s.\\
%
Another problem that may be solved offline, is the computation of the path to the minimum. This is necessary because the robot may go through local peaks of maximum reflected mass, on his trajectory to the global minimum. This may be solved with path optimization techniques, out of the scope of this work. 

%
One way to see the numerical values of the grid is as image-points of a picture. Where the third coordinate of each point represents a property of the image at that point, like for example the color. From this point of view the dataset, that is the point cloud, may be treated with tools from the MATLAB Image Processing Toolbox. These can be used to write an algorithm that finds the minima in the data set. Unfortunately the DLR does not have a license for such a Toolbox so other implementation is necessary.
Without making use of this Toolbox a filter \cite{minmaxfilter} to find the local maxima and minima from a given dataset is used. This filter computes the maxima and minima over running windows of a determined size. The concept “\textit{window}” here refers to the image-processing domain. Given a pixel value at a point \textit{(x,y)} the window limits an area (usually squared) around the pixel, where the extreme local values are searched. The \textit{pixel} is in our case  the value of the reflected mass.\\
%
In the filter, the sliding window can be a scalar (in which case the same window size will be scanned for all dimensions) or an array (in which case separate scan sizes will be run for each dimension). The advantage of this filter is that it can do multidimensional filtering. Here the signal is two dimensional, i.e. with two independent variables $q_1$ and $q_4$. Therefore the window-size should have two entries (or 1 if one wants unidimensional filtering).


Here, two examples are explained for better understanding:
\begin{enumerate}
	\item In the first case (Fig. \ref{fig:maxminfilter1}) the window size is set to $[dim_{max_{1}}, \ 1]$. The local minimum is found in each self-motion manifold slice, for constant positions of the elbow.
	\item In the second case (Fig. \ref{fig:maxminfilter2}) the window size is set to $[1, \ dim_{max_{2}}]$. The local minimum is found in each self-motion manifold slice, for constant positions of the linear axis.
	
\end{enumerate}

Where $dim_{max_{i}}$ is the data length of the $ith$ dimension. It is clear to see that when setting both dimensions to their relative maximim values, $[dim_{max_{1}}, \ dim_{max_{2}}]$, only the global extrema are found.

\textcolor{red}{INCREASE THE FONT OF THE TITTLE AND AXIS}

\begin{figure}[htb]
	\centerline{
		\includegraphics[width=1\textwidth]{images/global_q4_cte_v2.eps}}
	\caption{Plot of the reflected mass in x direction against the self-motion
		manifold coordinates. Local minima found in each self-motion manifold slice for each elbow position. }
	\label{fig:maxminfilter1}
\end{figure}

\begin{figure}[htb]
	\centerline{
		\includegraphics[width=1.0\textwidth]{images/global_q1_cte_v2.eps}}
	\caption{Plot of the reflected mass in x direction against the self-motion
		manifold coordinates. Local minima found in each self-motion manifold slice for each linear axis position. }
	\label{fig:maxminfilter2}
\end{figure}





\paragraph{The Algorithm}
\label{subsec:global_alg}

The filter is based on a \textit{Monotonic Wedge} algorithm \cite{Lemire}.
A simpler algorithm is the \textit{Max Queue} algorithm. In this last one, there is a queue with a fixed capacity and values are pushed into it. When a value is pushed another one is pulled. The values are continuously pushed while the maximum from the queue is written in a results-row.
It uses the separability of the problem.  For example in a two dimensional grid, it goes through all the rows and then through all the columns, making it two separate one dimensional problems.\\
%
Although intuitive and easy to implement it does not work that well with pseudo-monotonic data. The null space motion grids of the reflected mass may be divided into pseudo monotonic areas. The \textit{Monotonic Wedge} algorithm performs better in these cases, therefore this algorithm is preferred to implement here. This algorithm also ensures that new maximums are quickly found when moving out of the window. However, as we are optimizing offline this is not a defining advantage.
%Although the speed is not a relevant factor as we are treating the data offline.

%Like before the values of the reflected mass are being pushed and pulled. But instead of searching the elements through the whole window the algorithm just takes the next element from the wedge.

%\underline{Short introduction to wedges:} Let \textbf{$U_i$}  be a monotonic wedge of the sequence \textbf{$A_i$} . Then \textbf{$U_i$}  has the following properties:

%\textbf{$U_0$} is the index of the maximum element of \textbf{$A_{U_0}$} ,

%U1  is the largest element of all elements to the right of \textbf{$A_{U_0}$} .
%In general, \textbf{$U_k$}  is the largest element of all elements to the right of \textbf{$A_{U_{k-1}}$}.
%{\color{red} add more explanation and references about this?? if needed see more explanation and other 2D approaches in the pdf minmaxfilters.pdf. IT WOULD BE INTERESTING COMPARING THE OTHER APPROACHES AND SAYING WHY I DECIDED FOR THIS ONE!!!!}

%The advantage of this algorithm is mainly the speed {\color{red} FIND A BETTER ADVANTAGE because if this approach is only for offline optimization actually it does not important that much the speed!! }  because it requires, in the worst case, 3 comparisons per element.

The standard way to find the minima of a given grid is with sliding windows of small length in both dimensions. A more accurate way is: First finding all the minima for the self-motion manifold slice, corresponding to each position of the elbow (see Fig.\ref{fig:maxminfilter1}). Secondly doing an equivalent minimization for each position of the linear axis (see Fig.\ref{fig:maxminfilter2}). 
And finally get the common minima from both sets. This strategy was implemented, and performs better than the mentioned standard way, in all tested scenarios. 

\paragraph{Continuity of the Global Minimum}

A requirement for the goal position is its continuity. While following a trajectory the different joint configurations lead to different grids. As long as the direction does not change abruptly (and this is the case considered in this work), the minimum shows continuity. In the sense that every computed global minimum does not lay far from the previous one. An exception occurs when the grid shows contour lines. If contour lines exist, where local minima lay. And the global minimum is in this contour line. Due to small computational errors where computing the grid, the global minimum suffer of discontinuity. In the tests run, when the robot is performing a null space motion, the global minimum is computed in different points of the contour line. Meaning that the global minimum is prone to discontinuities. This makes the global minimum an undesired goal position.
It is also worth mentioning that in y- and z- direction the grid shows in all the tested scenarios contour lines where the global minimum is included. Meaning that in these directions local minimization provides as good results, in terms of optimization, as the global minimization. 




















\subsection{Local Optimization}
\label{ch:localoptimization}
\label{sec:localoptimization}


This section considers the search of the 
local minimum in the reflected mass, for setting it as the goal position in the attractive potential from (\ref{eq:potential_intro}). Classical local minimization techniques can be divided  into Trust Region and Line Search   techniques. In this section a review on both is provided.



\subsubsection{Iterative Strategies}
\label{sec:iterative_strategies}


\paragraph{Trust Region}
\label{subsec:tregion}

Given an objective function to minimize. This  strategy firstly aims to obtain a subset of the region of this objective function. The subset has to contain the current point. Secondly it finds a minimum in this subset. The subset is often approximated using a quadratic function. This approximated function has a similar behavior to the original objective function in the vicinity of the current point. If the trusted region is small enough, a step size will be found that decreases the objective function sufficiently. This makes sense because for general nonlinear functions a local approximation (like linear approximation or quadratic approximation)  is only locally valid. \\ 
%
One of the requirements is then an analytical objective function, which will be locally approximated. In this case no analytic solution has been found (see Section \ref{subsec:imposing_ns_constraint}) which  provides all the possible null space positions, i.e. there is no analytical form of the objective function.\\
%
%In Section \ref{sec:global_analytical} an attempt was done to find this analytical form using curve fitting. But first this required the point cloud of the data, so it suited only global minimization. And second this objective function is not independent of the direction of motion or the initial joint configuration.
%
Another approach would be computing only a subset of the grid in a small region enclosing the current point. And find an analytical form which approximates the shape of this region. However, once the points are computed, it would be easier to compare the masses of these points than creating a function that would contain them in order to apply the Trust Region method. Therefore these methods are of no interest for this work.





\paragraph{Line Search}
\label{subsec:lsearch}



In the these techniques, starting from the current iteration, a descent direction is chosen in order to minimize the objective function. Then the step size is computed to determine the distance to move in this direction. In each iteration the direction and the step size need to be computed. The descent direction can be computed by various methods, such as gradient descent (GD), Newton methods (NM) or Quasi-Newton methods (QNM). % And the step size can be determined either exactly or inexactly.
Both Line Search methods and Trust Region methods generate a step using a model of the objective function. Therefore the lack of an analytical form as objective function arises here as in the Trust Region strategy.
However, while the Trust Region methods use this model to generate a region, the Line Search methods use it to compute the descent direction.  
This problem of finding the descent direction can be addressed as finding null space velocity that minimizes the reflected mass. This velocity may be selected by a  select-function :
%
\begin{align}
\text{select}: \mathbf{C}\rightarrow\ker(\mathbf{J}(\mathbf{q})),\; \mathbf{q}\mapsto\dot{\mathbf{q}} .
\end{align}
%
 The search of a null space velocity that minimizes the reflected mass is treated in the next chapter. 








